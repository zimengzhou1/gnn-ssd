{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os.path as osp\n",
    "import os\n",
    "import argparse\n",
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "__file__ = os.path.abspath('')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset_path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', 'Reddit')\n",
    "dataset = Reddit(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[232965, 602], edge_index=[2, 114615892], y=[232965], train_mask=[232965], val_mask=[232965], test_mask=[232965])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating coo/csc/csr format of dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Construct sparse formats\n",
    "print('Creating coo/csc/csr format of dataset...')\n",
    "num_nodes = dataset[0].num_nodes\n",
    "coo = dataset[0].edge_index.numpy()\n",
    "v = np.ones_like(coo[0])\n",
    "coo = scipy.sparse.coo_matrix((v, (coo[0], coo[1])), shape=(num_nodes, num_nodes))\n",
    "csc = coo.tocsc()\n",
    "csr = coo.tocsr()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving indptr...\n",
      "Done!\n",
      "Saving indices...\n",
      "Done!\n",
      "Saving features...\n",
      "Done!\n",
      "Saving labels...\n",
      "Done!\n",
      "Making conf file...\n",
      "Done!\n",
      "Saving split index...\n",
      "Done!\n",
      "Calculating score for neighbor cache construction...\n",
      "Done!\n",
      "Saving score...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save csc-formatted dataset\n",
    "indptr = csc.indptr.astype(np.int64)\n",
    "indices = csc.indices.astype(np.int64)\n",
    "features = torch.rand(dataset[0].x.size(0), 2048) #dataset[0].x\n",
    "labels = dataset[0].y\n",
    "\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "indptr_path = os.path.join(dataset_path, 'indptr.dat')\n",
    "indices_path = os.path.join(dataset_path, 'indices.dat')\n",
    "features_path = os.path.join(dataset_path, 'features.dat')\n",
    "labels_path = os.path.join(dataset_path, 'labels.dat')\n",
    "conf_path = os.path.join(dataset_path, 'conf.json')\n",
    "split_idx_path = os.path.join(dataset_path, 'split_idx.pth')\n",
    "\n",
    "print('Saving indptr...')\n",
    "indptr_mmap = np.memmap(indptr_path, mode='w+', shape=indptr.shape, dtype=indptr.dtype)\n",
    "indptr_mmap[:] = indptr[:]\n",
    "indptr_mmap.flush()\n",
    "print('Done!')\n",
    "\n",
    "print('Saving indices...')\n",
    "indices_mmap = np.memmap(indices_path, mode='w+', shape=indices.shape, dtype=indices.dtype)\n",
    "indices_mmap[:] = indices[:]\n",
    "indices_mmap.flush()\n",
    "print('Done!')\n",
    "\n",
    "print('Saving features...')\n",
    "features_mmap = np.memmap(features_path, mode='w+', shape=features.shape, dtype=np.float32)\n",
    "features_mmap[:] = features[:]\n",
    "features_mmap.flush()\n",
    "print('Done!')\n",
    "\n",
    "print('Saving labels...')\n",
    "labels = labels.type(torch.float32)\n",
    "labels_mmap = np.memmap(labels_path, mode='w+', shape=dataset[0].y.shape, dtype=np.float32)\n",
    "labels_mmap[:] = labels[:]\n",
    "labels_mmap.flush()\n",
    "print('Done!')\n",
    "\n",
    "print('Making conf file...')\n",
    "mmap_config = dict()\n",
    "mmap_config['num_nodes'] = int(dataset[0].num_nodes)\n",
    "mmap_config['indptr_shape'] = tuple(indptr.shape)\n",
    "mmap_config['indptr_dtype'] = str(indptr.dtype)\n",
    "mmap_config['indices_shape'] = tuple(indices.shape)\n",
    "mmap_config['indices_dtype'] = str(indices.dtype)\n",
    "mmap_config['indices_shape'] = tuple(indices.shape)\n",
    "mmap_config['indices_dtype'] = str(indices.dtype)\n",
    "mmap_config['indices_shape'] = tuple(indices.shape)\n",
    "mmap_config['indices_dtype'] = str(indices.dtype)\n",
    "mmap_config['features_shape'] = tuple(features_mmap.shape)\n",
    "mmap_config['features_dtype'] = str(features_mmap.dtype)\n",
    "mmap_config['labels_shape'] = tuple(labels_mmap.shape)\n",
    "mmap_config['labels_dtype'] = str(labels_mmap.dtype)\n",
    "mmap_config['num_classes'] = int(dataset.num_classes)\n",
    "json.dump(mmap_config, open(conf_path, 'w'))\n",
    "print('Done!')\n",
    "\n",
    "print('Saving split index...')\n",
    "splits = {'train': dataset[0].train_mask, 'test': dataset[0].test_mask, 'valid': dataset[0].val_mask}\n",
    "torch.save(splits, split_idx_path)\n",
    "print('Done!')\n",
    "\n",
    "# Calculate and save score for neighbor cache construction\n",
    "print('Calculating score for neighbor cache construction...')\n",
    "score_path = os.path.join(dataset_path, 'nc_score.pth')\n",
    "csc_indptr_tensor = torch.from_numpy(csc.indptr.astype(np.int64))\n",
    "csr_indptr_tensor = torch.from_numpy(csr.indptr.astype(np.int64))\n",
    "\n",
    "eps = 0.00000001\n",
    "in_num_neighbors = (csc_indptr_tensor[1:] - csc_indptr_tensor[:-1]) + eps\n",
    "out_num_neighbors = (csr_indptr_tensor[1:] - csr_indptr_tensor[:-1]) + eps\n",
    "score = out_num_neighbors / in_num_neighbors\n",
    "print('Done!')\n",
    "\n",
    "print('Saving score...')\n",
    "torch.save(score, score_path)\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mmap_dataset(path='../data/Reddit'):\n",
    "    indptr_path = os.path.join(path, 'indptr.dat')\n",
    "    indices_path = os.path.join(path, 'indices.dat')\n",
    "    features_path = os.path.join(path, 'features.dat')\n",
    "    labels_path = os.path.join(path, 'labels.dat')\n",
    "    conf_path = os.path.join(path, 'conf.json')\n",
    "    split_idx_path = os.path.join(path, 'split_idx.pth')\n",
    "\n",
    "    conf = json.load(open(conf_path, 'r'))\n",
    "\n",
    "    # Assume we only memmap for large files - the adjacency matrix (indices) + features ~ 13GB and 50GB respectively\n",
    "\n",
    "    indptr = np.fromfile(indptr_path, dtype=conf['indptr_dtype']).reshape(tuple(conf['indptr_shape']))\n",
    "    indices = np.memmap(indices_path, mode='r', shape=tuple(conf['indices_shape']), dtype=conf['indices_dtype'])\n",
    "    print(indptr)\n",
    "    features_shape = conf['features_shape']\n",
    "    features = np.memmap(features_path, mode='r', shape=tuple(features_shape), dtype=conf['features_dtype'])\n",
    "    labels = np.fromfile(labels_path, dtype=conf['labels_dtype'], count=conf['num_nodes']).reshape(tuple([conf['labels_shape'][0]]))\n",
    "\n",
    "    indptr = torch.from_numpy(indptr)\n",
    "    indices = torch.from_numpy(indices)\n",
    "    features = torch.from_numpy(features)\n",
    "    labels = torch.from_numpy(labels)\n",
    "\n",
    "    num_nodes = conf['num_nodes']\n",
    "    num_features = conf['features_shape'][1]\n",
    "    num_classes = conf['num_classes']\n",
    "\n",
    "    split_idx = torch.load(split_idx_path)\n",
    "    train_idx = split_idx['train']\n",
    "    val_idx = split_idx['valid']\n",
    "    test_idx = split_idx['test']\n",
    "\n",
    "    return indptr, indices, features, labels, num_features, num_classes, num_nodes, train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[        0      2204      2358 ... 114615262 114615401 114615892]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1827494/149090783.py:21: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/torch/csrc/utils/tensor_numpy.cpp:172.)\n",
      "  indices = torch.from_numpy(indices)\n"
     ]
    }
   ],
   "source": [
    "indptr, indices, x, y, num_features, num_classes, num_nodes, train_idx, valid_idx, test_idx = get_mmap_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(153431)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      for conv in self.convs:\n",
    "        conv.reset_parameters()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, subgraph_loader):\n",
    "        pbar = tqdm(total=len(subgraph_loader.dataset) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch:\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
    "                x = conv(x, batch.edge_index.to(device))\n",
    "                if i < len(self.convs) - 1:\n",
    "                    x = x.relu_()\n",
    "                xs.append(x[:batch.batch_size].cpu())\n",
    "                pbar.update(batch.batch_size)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        pbar.close()\n",
    "        return x_all\n",
    "\n",
    "model = SAGE(num_features, 256, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customNeighborSampler import MMAPNeighborSampler\n",
    "from lib.utils import *\n",
    "\n",
    "size = \"10,10\"\n",
    "sizes = [int(size) for size in size.split(',')]\n",
    "train_loader = MMAPNeighborSampler(indptr, indices, node_idx=train_idx,\n",
    "                               sizes=sizes, batch_size=1000,\n",
    "                               shuffle=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232965"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.node_idx.view(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/153431 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "pbar = tqdm(total=int(sum(train_idx)))\n",
    "for step, (batch_size, ids, adjs) in enumerate(train_loader):\n",
    "    batch_inputs = gather_mmap(x, ids)\n",
    "    batch_labels = y[ids[:batch_size]]\n",
    "\n",
    "    # Transfer\n",
    "    batch_inputs_cuda = batch_inputs.to(device)\n",
    "    batch_labels_cuda = batch_labels.to(device)\n",
    "\n",
    "    adjs_cuda = [adj.to(device) for adj in adjs]\n",
    "\n",
    "    # Forward\n",
    "    out = model(batch_inputs_cuda, adjs_cuda)\n",
    "    loss = F.nll_loss(out, batch_labels_cuda.long())\n",
    "    pbar.update(batch_size)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153431"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73986a8f3bce3efd6c5bb7f072a0fcc3be48ec97f1ec1ace688ac1bc99e481cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
